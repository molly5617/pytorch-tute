{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Encoding Text Data\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Encoding Text Data\n",
    "# ###########################################################################################################\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('../../../notebooks/Data/shakespeare.txt', mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "5445609\n"
     ]
    }
   ],
   "source": [
    "print(type(text))\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     1\n",
      "  From fairest creatures we desire increase,\n",
      "  That thereby beauty's rose might never die,\n",
      "  But as the riper should by time decease,\n",
      "  His tender heir might bear his memory:\n",
      "  But thou contracted to thine own bright eyes,\n",
      "  Feed'st thy light's flame with self-substantial fuel,\n",
      "  Making a famine where abundance lies,\n",
      "  Thy self thy foe, to thy sweet self too cruel:\n",
      "  Thou that art now the world's fresh ornament,\n",
      "  And only herald to the gaudy spring,\n",
      "  Within thine own bud buriest thy content,\n",
      "  And tender churl mak'st waste in niggarding:\n",
      "    Pity the world, or else this glutton be,\n",
      "    To eat the world's due, by the grave and thee.\n",
      "\n",
      "\n",
      "                     2\n",
      "  When forty winters shall besiege thy brow,\n",
      "  And dig deep trenches in thy beauty's field,\n",
      "  Thy youth's proud livery so gazed on now,\n",
      "  Will be a tattered weed of small worth held:  \n",
      "  Then being asked, where all thy beauty lies,\n",
      "  Where all the treasure of thy lusty days;\n",
      "  To say within thine own deep su\n"
     ]
    }
   ],
   "source": [
    "print(text[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84\n",
      "{'P', '(', 'v', 'X', 'Y', 'r', 'd', 'u', '[', 'g', 'y', 'k', 'z', 'o', 'C', 'N', 'f', '}', '0', '6', 'F', 'W', '\"', ':', 'M', '>', 'U', 'c', ')', 'h', 'D', 'V', '<', '8', 't', 'K', '|', 'H', 'q', 'Q', 'I', '\\n', 'A', '.', 'p', 'w', '-', ',', 'n', '_', '5', 'x', '`', 'L', '2', '7', 'G', 'b', '9', '?', 'l', 'j', 'e', '&', 'J', 'm', '4', ' ', '!', 's', 'T', 'a', 'i', '3', 'Z', '1', 'O', ';', 'S', \"'\", 'R', 'E', ']', 'B'}\n"
     ]
    }
   ],
   "source": [
    "# Derive unique characters from the text.\n",
    "all_unique_chars = set(text)\n",
    "print(len(all_unique_chars))\n",
    "print(all_unique_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'P',\n",
       " 1: '(',\n",
       " 2: 'v',\n",
       " 3: 'X',\n",
       " 4: 'Y',\n",
       " 5: 'r',\n",
       " 6: 'd',\n",
       " 7: 'u',\n",
       " 8: '[',\n",
       " 9: 'g',\n",
       " 10: 'y',\n",
       " 11: 'k',\n",
       " 12: 'z',\n",
       " 13: 'o',\n",
       " 14: 'C',\n",
       " 15: 'N',\n",
       " 16: 'f',\n",
       " 17: '}',\n",
       " 18: '0',\n",
       " 19: '6',\n",
       " 20: 'F',\n",
       " 21: 'W',\n",
       " 22: '\"',\n",
       " 23: ':',\n",
       " 24: 'M',\n",
       " 25: '>',\n",
       " 26: 'U',\n",
       " 27: 'c',\n",
       " 28: ')',\n",
       " 29: 'h',\n",
       " 30: 'D',\n",
       " 31: 'V',\n",
       " 32: '<',\n",
       " 33: '8',\n",
       " 34: 't',\n",
       " 35: 'K',\n",
       " 36: '|',\n",
       " 37: 'H',\n",
       " 38: 'q',\n",
       " 39: 'Q',\n",
       " 40: 'I',\n",
       " 41: '\\n',\n",
       " 42: 'A',\n",
       " 43: '.',\n",
       " 44: 'p',\n",
       " 45: 'w',\n",
       " 46: '-',\n",
       " 47: ',',\n",
       " 48: 'n',\n",
       " 49: '_',\n",
       " 50: '5',\n",
       " 51: 'x',\n",
       " 52: '`',\n",
       " 53: 'L',\n",
       " 54: '2',\n",
       " 55: '7',\n",
       " 56: 'G',\n",
       " 57: 'b',\n",
       " 58: '9',\n",
       " 59: '?',\n",
       " 60: 'l',\n",
       " 61: 'j',\n",
       " 62: 'e',\n",
       " 63: '&',\n",
       " 64: 'J',\n",
       " 65: 'm',\n",
       " 66: '4',\n",
       " 67: ' ',\n",
       " 68: '!',\n",
       " 69: 's',\n",
       " 70: 'T',\n",
       " 71: 'a',\n",
       " 72: 'i',\n",
       " 73: '3',\n",
       " 74: 'Z',\n",
       " 75: '1',\n",
       " 76: 'O',\n",
       " 77: ';',\n",
       " 78: 'S',\n",
       " 79: \"'\",\n",
       " 80: 'R',\n",
       " 81: 'E',\n",
       " 82: ']',\n",
       " 83: 'B'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number -> letter (Decoder takes the number values and return respective character - basically a\n",
    "# lookup dictionary)\n",
    "decoder = dict(enumerate(all_unique_chars))\n",
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'P': 0,\n",
       " '(': 1,\n",
       " 'v': 2,\n",
       " 'X': 3,\n",
       " 'Y': 4,\n",
       " 'r': 5,\n",
       " 'd': 6,\n",
       " 'u': 7,\n",
       " '[': 8,\n",
       " 'g': 9,\n",
       " 'y': 10,\n",
       " 'k': 11,\n",
       " 'z': 12,\n",
       " 'o': 13,\n",
       " 'C': 14,\n",
       " 'N': 15,\n",
       " 'f': 16,\n",
       " '}': 17,\n",
       " '0': 18,\n",
       " '6': 19,\n",
       " 'F': 20,\n",
       " 'W': 21,\n",
       " '\"': 22,\n",
       " ':': 23,\n",
       " 'M': 24,\n",
       " '>': 25,\n",
       " 'U': 26,\n",
       " 'c': 27,\n",
       " ')': 28,\n",
       " 'h': 29,\n",
       " 'D': 30,\n",
       " 'V': 31,\n",
       " '<': 32,\n",
       " '8': 33,\n",
       " 't': 34,\n",
       " 'K': 35,\n",
       " '|': 36,\n",
       " 'H': 37,\n",
       " 'q': 38,\n",
       " 'Q': 39,\n",
       " 'I': 40,\n",
       " '\\n': 41,\n",
       " 'A': 42,\n",
       " '.': 43,\n",
       " 'p': 44,\n",
       " 'w': 45,\n",
       " '-': 46,\n",
       " ',': 47,\n",
       " 'n': 48,\n",
       " '_': 49,\n",
       " '5': 50,\n",
       " 'x': 51,\n",
       " '`': 52,\n",
       " 'L': 53,\n",
       " '2': 54,\n",
       " '7': 55,\n",
       " 'G': 56,\n",
       " 'b': 57,\n",
       " '9': 58,\n",
       " '?': 59,\n",
       " 'l': 60,\n",
       " 'j': 61,\n",
       " 'e': 62,\n",
       " '&': 63,\n",
       " 'J': 64,\n",
       " 'm': 65,\n",
       " '4': 66,\n",
       " ' ': 67,\n",
       " '!': 68,\n",
       " 's': 69,\n",
       " 'T': 70,\n",
       " 'a': 71,\n",
       " 'i': 72,\n",
       " '3': 73,\n",
       " 'Z': 74,\n",
       " '1': 75,\n",
       " 'O': 76,\n",
       " ';': 77,\n",
       " 'S': 78,\n",
       " \"'\": 79,\n",
       " 'R': 80,\n",
       " 'E': 81,\n",
       " ']': 82,\n",
       " 'B': 83}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# letter -> number (Encoder takes a letter and return the corresponding number for it)\n",
    "encoder = {char: idx for idx, char in decoder.items()} # Dictionary generator.\n",
    "encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([41, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67, 67,\n",
       "       67, 67, 67, 67, 67, 75, 41, 67, 67, 20,  5, 13, 65, 67, 16, 71, 72,\n",
       "        5, 62, 69, 34, 67, 27,  5, 62, 71, 34,  7,  5, 62, 69, 67, 45, 62,\n",
       "       67,  6, 62, 69, 72,  5, 62, 67, 72, 48, 27,  5, 62, 71, 69, 62, 47,\n",
       "       41, 67, 67, 70, 29, 71, 34, 67, 34, 29, 62,  5, 62, 57, 10, 67, 57,\n",
       "       62, 71,  7, 34, 10, 79, 69, 67,  5, 13, 69, 62, 67, 65, 72,  9, 29,\n",
       "       34, 67, 48, 62,  2, 62,  5, 67,  6, 72, 62, 47, 41, 67, 67, 83,  7,\n",
       "       34, 67, 71, 69, 67, 34, 29, 62, 67,  5, 72, 44, 62,  5, 67, 69, 29,\n",
       "       13,  7, 60,  6, 67, 57, 10, 67, 34, 72, 65, 62, 67,  6, 62, 27, 62,\n",
       "       71, 69, 62, 47, 41, 67, 67, 37, 72, 69, 67, 34, 62, 48,  6, 62,  5,\n",
       "       67, 29, 62, 72,  5, 67, 65, 72,  9, 29, 34, 67, 57, 62, 71,  5, 67,\n",
       "       29, 72, 69, 67, 65, 62, 65, 13,  5, 10, 23, 41, 67, 67, 83,  7, 34,\n",
       "       67, 34, 29, 13,  7, 67, 27, 13, 48, 34,  5, 71, 27, 34, 62,  6, 67,\n",
       "       34, 13, 67, 34, 29, 72, 48, 62, 67, 13, 45, 48, 67, 57,  5, 72,  9,\n",
       "       29, 34, 67, 62, 10, 62, 69, 47, 41, 67, 67, 20, 62, 62,  6, 79, 69,\n",
       "       34, 67, 34, 29, 10, 67, 60, 72,  9, 29, 34, 79, 69, 67, 16, 60, 71,\n",
       "       65, 62, 67, 45, 72, 34, 29, 67, 69, 62, 60, 16, 46, 69,  7, 57, 69,\n",
       "       34, 71, 48, 34, 72, 71, 60, 67, 16,  7, 62, 60, 47, 41, 67, 67, 24,\n",
       "       71, 11, 72, 48,  9, 67, 71, 67, 16, 71, 65, 72, 48, 62, 67, 45, 29,\n",
       "       62,  5, 62, 67, 71, 57,  7, 48,  6, 71, 48, 27, 62, 67, 60, 72, 62,\n",
       "       69, 47, 41, 67, 67, 70, 29, 10, 67, 69, 62, 60, 16, 67, 34, 29, 10,\n",
       "       67, 16, 13, 62, 47, 67, 34, 13, 67, 34, 29, 10, 67, 69, 45, 62, 62,\n",
       "       34, 67, 69, 62, 60, 16, 67, 34, 13, 13, 67, 27,  5,  7, 62, 60, 23,\n",
       "       41, 67, 67, 70, 29, 13,  7, 67, 34, 29, 71, 34, 67, 71,  5, 34, 67,\n",
       "       48, 13, 45, 67, 34, 29, 62, 67, 45, 13,  5, 60,  6, 79, 69, 67, 16,\n",
       "        5, 62, 69, 29, 67, 13,  5, 48, 71, 65, 62, 48, 34, 47, 41, 67, 67,\n",
       "       42, 48,  6, 67, 13, 48, 60, 10, 67, 29, 62,  5, 71, 60,  6, 67, 34,\n",
       "       13, 67, 34, 29, 62, 67,  9, 71,  7,  6, 10, 67, 69, 44,  5, 72, 48,\n",
       "        9, 47, 41, 67, 67, 21, 72, 34, 29, 72, 48, 67, 34, 29, 72, 48, 62,\n",
       "       67, 13, 45, 48, 67, 57,  7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the entire text with encoder.\n",
    "encoded_text = np.array([encoder[char] for char in text])\n",
    "encoded_text[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Declare a function for one-hot encoding.\n",
    "def one_hot_encoder(encoded_text, number_of_unique_chars):\n",
    "\n",
    "    # encoded_text -> batch of encoded text\n",
    "    # number_of_unique_chars -> len(set(text))\n",
    "\n",
    "    one_hot = np.zeros((encoded_text.size, number_of_unique_chars), dtype=np.float32)\n",
    "    # OR use \"one_hot = one_hot.astype(np.float32)\"\n",
    "    # Use the 'dtype=float32' to get precision on PyTorch.\n",
    "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.\n",
    "    # FANCY INDEXING = Passing an array of indices to access multiple array elements at once.\n",
    "    one_hot = one_hot.reshape((*encoded_text.shape, number_of_unique_chars)) # Not mandatory.\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 0 1 3]\n",
      "[[0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "ex = np.array([1, 2, 0, 1, 3])\n",
    "print(ex)\n",
    "print(one_hot_encoder(ex, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oh = np.zeros((5, 4), dtype=np.float32)\n",
    "# FANCY INDEXING = Passing an array of indices to access multiple array elements at once.\n",
    "oh[np.arange(oh.shape[0]), ex.flatten()] = 1.\n",
    "oh.reshape(5, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Generating Training Batches\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Generating Training Batches\n",
    "# ###########################################################################################################\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batches(encoded_text, sample_per_batch=10, sequence_length=50):\n",
    "\n",
    "    # X : 'encoded_text' of length 'sequence_length' -> [0, 1, 2], [1, 2, 3]\n",
    "    # Y : 'encoded_text' shifted by one to right.    -> [1, 2, 3], [2, 3, 4] : shifted to right by one index.\n",
    "\n",
    "    # Number of characters per batch\n",
    "    chars_per_batch = sample_per_batch * sequence_length\n",
    "    # Number of batches available.\n",
    "    number_of_batches = int(len(encoded_text) / chars_per_batch)\n",
    "\n",
    "    # Cut-off the end of the encoded_text that won't fit evenly into a batch (Loss little bit info).\n",
    "    encoded_text = encoded_text[:number_of_batches * chars_per_batch]\n",
    "\n",
    "    encoded_text = encoded_text.reshape(sample_per_batch, -1)\n",
    "\n",
    "    for n in range(0, encoded_text.shape[1], sequence_length):\n",
    "\n",
    "        x = encoded_text[:, n:n + sequence_length]\n",
    "        # Create a 'x' like zeros array.\n",
    "        y = np.zeros_like(x)\n",
    "\n",
    "        try:\n",
    "\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, n + sequence_length]\n",
    "\n",
    "        except IndexError:\n",
    "\n",
    "            y[:, :-1] = x[:, 1:]\n",
    "            y[:, -1] = encoded_text[:, 0]\n",
    "\n",
    "        yield  x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sample = np.arange(20)\n",
    "my_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0,  1,  2,  3,  4],\n",
       "        [10, 11, 12, 13, 14]]),\n",
       " array([[ 1,  2,  3,  4,  5],\n",
       "        [11, 12, 13, 14, 15]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_batch_generator = generate_batches(my_sample, sample_per_batch=2, sequence_length=5)\n",
    "x, y = next(my_batch_generator)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Creating the LSTM Model\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Creating the LSTM Model\n",
    "# ###########################################################################################################\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CharacterModel(nn.Module):\n",
    "\n",
    "    def __init__(self, all_unique_chars, hidden_size=256, num_layers=4, drop_p=0.5, use_gpu=False):\n",
    "        super().__init__()\n",
    "        self.drop_p = drop_p\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.use_gpu = use_gpu\n",
    "\n",
    "        self.all_chars = all_unique_chars\n",
    "        self.decoder = dict(enumerate(all_unique_chars))\n",
    "        self.encoder = {char: idx for idx, char in self.decoder.items()}\n",
    "\n",
    "        self.lstm = nn.LSTM(len(all_unique_chars),\n",
    "                            hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            dropout=drop_p,\n",
    "                            batch_first=True)\n",
    "        self.dropout = nn.Dropout(drop_p)\n",
    "        self.fc_linear = nn.Linear(hidden_size, len(all_unique_chars))\n",
    "\n",
    "    def forward(self, X, hidden):\n",
    "\n",
    "        lstm, hidden = self.lstm(X, hidden)\n",
    "        lstm = self.dropout(lstm)\n",
    "        lstm = lstm.contiguous().view(-1, self.hidden_size)\n",
    "        return self.fc_linear(lstm), hidden\n",
    "\n",
    "    def hidden_state(self, batch_size):\n",
    "\n",
    "        if self.use_gpu:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda(),\n",
    "                      torch.zeros(self.num_layers, batch_size, self.hidden_size).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.num_layers, batch_size, self.hidden_size),\n",
    "                      torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CharacterModel(\n",
       "  (lstm): LSTM(84, 512, num_layers=3, batch_first=True, dropout=0.5)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc_linear): Linear(in_features=512, out_features=84, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CharacterModel(all_unique_chars, hidden_size=512, num_layers=3).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5470292"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "for p in model.parameters():\n",
    "    total += p.numel()\n",
    "total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4901048"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_percent = 0.9\n",
    "train_idx = int(len(encoded_text) * train_percent)\n",
    "train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "training_data = encoded_text[:train_idx]\n",
    "test_data = encoded_text[train_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Training LSTM Model\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Training LSTM Model\n",
    "# ###########################################################################################################\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 60\n",
    "batch_size = 100\n",
    "seq_length = 100\n",
    "\n",
    "tracker = 0\n",
    "\n",
    "num_chars = max(encoded_text) + 1 # + 1 because indexes start at zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Set to training mode.\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-44-854abe398ced>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m                 \u001b[0mval_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mval_hidden\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m                 \u001b[0mlstm\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mval_hidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-cfc4224311b1>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, X, hidden)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mlstm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlstm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[0mlstm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlstm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorchdev\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\pytorchdev\\lib\\site-packages\\torch\\nn\\modules\\rnn.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck_forward_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    568\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 569\u001b[1;33m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0m\u001b[0;32m    570\u001b[0m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0;32m    571\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input and parameter tensors are not at the same device, found input tensor at cpu and parameter tensor at cuda:0"
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    hidden = model.hidden_state(batch_size)\n",
    "\n",
    "    for x, y in generate_batches(encoded_text, batch_size, seq_length):\n",
    "\n",
    "        tracker += 1\n",
    "\n",
    "        x = one_hot_encoder(x, num_chars)\n",
    "\n",
    "        x = torch.tensor(x, device=device)\n",
    "        target = torch.tensor(y, device=device)\n",
    "\n",
    "        # Reset the hidden state. to avoid back propagation of hidden layer.\n",
    "        hidden = tuple([state.data.to(device) for state in hidden])\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        lstm, hidden = model(x, hidden)\n",
    "        loss = criterion(lstm, target.view(batch_size * seq_length).long())\n",
    "\n",
    "        # Back-propagate before 'gradient clipping'.\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping to avoid gradient explosions.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        if tracker % 25 == 0:\n",
    "\n",
    "            val_hidden = model.hidden_state(batch_size)\n",
    "\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "\n",
    "            for x, y in generate_batches(test_data, batch_size, seq_length):\n",
    "\n",
    "                x = one_hot_encoder(x, num_chars)\n",
    "\n",
    "                x = torch.tensor(x)\n",
    "                target = torch.tensor(y)\n",
    "\n",
    "                if model.use_gpu:\n",
    "                    x = x.cuda()\n",
    "                    target = target.cuda()\n",
    "\n",
    "                val_hidden = tuple([state.data for state in val_hidden])\n",
    "                lstm , val_hidden = model(x, val_hidden)\n",
    "\n",
    "                val_loss = criterion(lstm, target.view(batch_size * seq_length).long())\n",
    "\n",
    "                val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            print(f'epoch: {i}, step: {tracker} -> validation loss -> {val_loss.item()}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Save the trained model.\n",
    "model_name = 'shakespeare_model.net___'\n",
    "torch.save(model.state_dict(), model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "# ###########################################################################################################\n",
      "#  NLP with PyTorch - Generating Predictions\n",
      "# ###########################################################################################################\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('''\n",
    "# ###########################################################################################################\n",
    "#  NLP with PyTorch - Generating Predictions\n",
    "# ###########################################################################################################\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change 'map_location' to CPU hence the model trained with GPU.\n",
    "model = model.load_state_dict(torch.load(f='shakespeare_model.net', map_location=torch.device('cpu')))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def predict_next_char(model, char, hidden=None, k=1):\n",
    "\n",
    "    encoded_text = model.encoder[char]\n",
    "\n",
    "    encoded_text = np.array([[encoded_text]])\n",
    "\n",
    "    encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
    "\n",
    "    inputs = torch.tensor(encoded_text)\n",
    "\n",
    "    # if model.use_gpu:\n",
    "    #     inputs = inputs.cuda()\n",
    "\n",
    "    hidden = tuple([state.data for state in model.hidden])\n",
    "\n",
    "    lstm_out, hidden = model(inputs, hidden)\n",
    "\n",
    "    probs = F.softmax(lstm_out, dim=1)\n",
    "\n",
    "    # If the network is using a GPU for training and validation, probabilities returns by the model\n",
    "    # should move back to the CPU, in order to further use with numpy.\n",
    "    if model .use_gpu:\n",
    "        probs = probs.cpu()\n",
    "\n",
    "    # 'topk(int)' we can use to define how many top matching outputs we need consider from the probability\n",
    "    # tensor\n",
    "    probs, index_positions = probs.topk(k)\n",
    "\n",
    "    index_positions = index_positions.numpy().squeeze()\n",
    "    probs = probs.numpy().flatten()\n",
    "    # Probabilities per index.\n",
    "    probs = probs / probs.sum()\n",
    "\n",
    "    char = np.random.choice(index_positions, p=probs)\n",
    "\n",
    "    return model.decoder[char], hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def generate_text(model, future_predicts, seed='The', k=1):\n",
    "\n",
    "    # if model.use_gpu:\n",
    "    #     model = model.cuda()\n",
    "    # else:\n",
    "    #     model = model.cpu()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    output_chars = [c for c in seed]\n",
    "    hidden = model.hidden_state(batch_size=1)\n",
    "\n",
    "    for char in seed:\n",
    "        char, hidden = predict_next_char(model, char, hidden, k=k)\n",
    "\n",
    "    output_chars.append(char)\n",
    "\n",
    "    for i in range(future_predicts):\n",
    "        char, hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
    "\n",
    "        output_chars.append(char)\n",
    "\n",
    "    return ''.join(output_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_IncompatibleKeys' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-162-3bae4013fc9b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerate_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'The'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-161-c30696244e26>\u001b[0m in \u001b[0;36mgenerate_text\u001b[1;34m(model, future_predicts, seed, k)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;31m#     model = model.cpu()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moutput_chars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mseed\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: '_IncompatibleKeys' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, 1000, seed='The', k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# This lecture series (NLP) is not completely done due to the performance issue in the laptop.\n",
    "# Must re-evaluate and try alone to the same or in a different approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
